# -*- coding: utf-8 -*-
"""NLP-twitter-sentimental-analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lPtQYJxkQwymccBBkteHIFKVAmIKcSzY

<font size="12"><font color='Blue'>Overview</font>
    
**Goal Of The Project:**
> Twitter sentiment analysis is a text analysis technique that involves determining the emotional tone or sentiment expressed in tweets posted on the social media platform Twitter. By analyzing the language and context of tweets, sentiment analysis aims to classify them as positive, negative, or neutral, providing valuable insights into public opinion, brand perception, and trends. This process often involves natural language processing (NLP) and machine learning algorithms to automate sentiment classification and extract meaningful information from the vast amount of data generated on Twitter. Such analysis is widely used in social media monitoring, marketing, and research to understand and respond to public sentiment effectively.

**Regarding Dataset:**
Twitter messages, entities, and sentiment make up the dataset. The dataset has three classes: neutral, negative, and positive.

* Tweet ID: The tweet's ID
* Entity: Entity that is mentioned in the tweet
* Sentiment: The tweet text's sentiment, whether positive, negative, neutral.
* Tweet Text as Tweet Content
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('twitter_training.csv'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

#importing libraries
import numpy as np
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import RegexpTokenizer

## ML Modelling ##
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

#load the data
train_df = pd.read_csv('twitter_training.csv',header= None)
train_df.columns =['Id', 'Entity', 'Sentiment', 'Tweet']
train_df['Sentiment'] = train_df['Sentiment'].replace('Irrelevant', 'Neutral')
train_df.head()

test_df = pd.read_csv('twitter_validation.csv',header= None)
test_df.columns =['Id', 'Entity', 'Sentiment', 'Tweet']
test_df['Sentiment'] = train_df['Sentiment'].replace('Irrelevant', 'Neutral')

""" <font size="6"><font color='Green'>Data Cleaning</font>"""

#checking for null values
train_df.isnull().sum()

"""The dataset currently contains 686 null values, so I'll be cleaning those up. Additionally, I'll be omitting all punctuation, numerals, and URLs to create a Tweet column that just contains words."""

#getting rid of null values
train_df=train_df.dropna()

# Define the cleaning_URLs function to get rid of URLs in the tweet
def cleaning_URLs(data):
    return re.sub('((www.[^s]+)|([^s]+.com)|(https?://[^s]+))','',data)

# Apply the cleaning_URLsto the tweet column
train_df['Tweet'] = train_df['Tweet'].apply(cleaning_URLs)

# Define the cleaning_numbers function to get rid of numbers in the tweet
def cleaning_numbers(data):
    return re.sub('[0-9]+', '', data)

# Apply the cleaning_numbers_udf to the clean_text column
train_df['Tweet'] = train_df['Tweet'].apply(cleaning_numbers)

# Define the cleaning_punctation function
def cleaning_punctation(data):
    return re.sub('[^a-zA-Z#]+', ' ', data)

# Apply the cleaning_numbers_udf to the clean_text column
train_df['Tweet'] = train_df['Tweet'].apply(cleaning_punctation)

#removing all emails in the text
pattern_to_remove = r'\S+@\S+'
# Use str.replace() to remove the pattern from the 'text_column'
train_df['Tweet'] = train_df['Tweet'].str.replace(pattern_to_remove, '', regex=True)

import nltk
nltk.download('stopwords')

def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return ' '.join(filtered_words)

train_df['Tweet'] = train_df['Tweet'].apply(remove_stopwords)

"""<font size="6"><font color='Green'>Exploratory data analysis</font>

**Pie Chart representation of sentiment distribution**
"""

# Count the occurrences of each category
category_counts = train_df['Sentiment'].value_counts()

# Plot a pie chart
plt.figure(figsize=(6, 8))
plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%', startangle=140)
plt.axis('equal')

plt.title('Distribution of Categories')
plt.show()

"""There are 27.9% positive sentiment texts, 30.2% negative sentiment texts, and 41.9% neutral sentiment texts in the training dataset

**Bar Chart representation of top 10 most engaged entites on tweets**
"""

# Count the occurrences of each category
entity_counts = train_df['Entity'].value_counts()

# Select the top 10 categories
top_10_entity = entity_counts.head(10)


# Plot a bar graph for the top 10 categories
plt.figure(figsize=(10, 6))
top_10_entity.plot(kind='bar', color='Grey')

for i, count in enumerate(entity_counts):
    plt.annotate(str(count), (i, count), ha='center', va='bottom')

plt.xlabel("Entity's")
plt.ylabel('Tweet Count')
plt.title('Top 10 Entity with Highest Tweet')
plt.xticks(rotation=90)
plt.show()

"""The top 10 entites which are listed lie almost in the same range MaddenNFl being the top most tweeted entity in the dataset

**Pie chart represntation of the top three entites in each sentiment category**
"""

# Group the data by 'tweet_category' and 'entity' and count occurrences
category_entity_counts = train_df.groupby(['Sentiment', 'Entity']).size().unstack(fill_value=0)

# Create a list to store the top 3 entities for each category
top_three_entities = []

# Iterate through each category
for category in category_entity_counts.index:
    top_entities = category_entity_counts.loc[category].nlargest(3)
    top_three_entities.append(top_entities)

# Create subplots for each category
plt.figure(figsize=(15, 5))
for i, (category, top_entities) in enumerate(zip(category_entity_counts.index, top_three_entities)):
    plt.subplot(1, 3, i + 1)
    plt.pie(top_entities, labels=top_entities.index, autopct='%1.1f%%', startangle=140)
    plt.axis('equal')
    plt.title(f'Top 3 Entities in {category}')

plt.tight_layout()
plt.show()

"""The pie chart compares all to three entites for each sentiment category. Wherein we see MaddenNFL has most negative tweets, Facebook remaning most neutral and AssassinsCreed with most postive tweets

<font size="6"><font color='Green'>Natural Language Processing</font>
"""

# Combine all the text into a single string
all_text = " ".join(train_df["Tweet"])

# Split the text into words and explode them into separate rows
word_list = all_text.split()
word_df = pd.DataFrame({"word": word_list})

# Count the frequency of each word and display the top 10
word_df_freq = word_df["word"].value_counts().reset_index()
word_df_freq.columns = ["word", "count"]
top_10_words = word_df_freq.head(10)

plt.figure(figsize=(8, 6))
plt.bar(top_10_words['word'], top_10_words['count'])
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title('Frequency of Words in Tweets')
plt.xticks(rotation=90)
plt.show()

# to find how many unique words in the vocabulary
unique_words_count = train_df['Tweet'].nunique()
print("Number of unique words:", unique_words_count)

# Extract text data from the 'text_column'
text_data = train_df['Tweet'].astype(str).str.cat(sep=' ')

# Generate the word cloud
wordcloud = WordCloud(width=800, height=800, background_color='white', min_font_size=10).generate(text_data)

# Display the word cloud
plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words="english")
bow = bow_vectorizer.fit_transform(train_df['Tweet'])

""" <font size="6"><font color='Green'>Model training and
   Prediction </font>
"""

#spliting the cleaned data into test and train
X_train, X_test, y_train, y_test = train_test_split(bow, train_df['Sentiment'], test_size=0.2, random_state=69)

#using naive bayes model to make predicitons
model_naive = MultinomialNB().fit(X_train,y_train)
pred = model_naive.predict(X_test)
print('Optimized Accuracy Score: {0: .3f}'.format(accuracy_score(y_test, pred)))

#custom function to calculate the models accuracy
def accuracy(pred, y_test):
    l=0
    for i,j in zip(pred, y_test):
        if i==j:
            l=l+1
    return l/len(pred)

#using Decision Tress Classifier to make prediction
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)
preds=model.predict(X_train)
accuracy(preds,y_train)

"""After training two models we have accuracy accordingly:
* Naive Bayes: 60.40%
* Decision Tree Classifier : 92.36%
"""

# ... (your existing code)

# Existing code continues...

bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words="english")
bow = bow_vectorizer.fit_transform(train_df['Tweet'])

#splitting the cleaned data into test and train
X_train, X_test, y_train, y_test = train_test_split(bow, train_df['Sentiment'], test_size=0.2, random_state=69)
#using naive bayes model to make predictions 
model_naive = MultinomialNB().fit(X_train, y_train)

# Function to predict sentiment using the model and vectorizer
def predict_sentiment(text):
    text_vectorized = bow_vectorizer.transform([text])
    prediction = model_naive.predict(text_vectorized)
    return prediction[0]

# GUI Section

import tkinter as tk
from tkinter import messagebox

def predict_sentiment_gui():
    text = entry.get()  # Get the input text from the entry field
    if text.strip() == '':
        messagebox.showwarning("Empty Input", "Please enter some text.")
    else:
        prediction = predict_sentiment(text)
        messagebox.showinfo("Sentiment Prediction", f"The predicted sentiment is: {prediction}")

# Create the main window
root = tk.Tk()
root.title("Sentiment Analysis")

# Create a label
label = tk.Label(root, text="Enter your text:")
label.pack()

# Create an entry field
entry = tk.Entry(root, width=50)  # Adjust width for better usability
entry.pack()

# Create a button to predict sentiment
predict_button = tk.Button(root, text="Predict", command=predict_sentiment_gui)
predict_button.pack()

# Function to handle Enter key press event
def on_return(event):
    predict_sentiment_gui()

# Bind Enter key to trigger prediction
root.bind('<Return>', on_return)

# Run the main loop
root.mainloop()

